---
layout: post
title: Supervised learning (지도 학습) - Machine Learning
excerpt: "Supervised learning is the machine learning task of learning a function that maps an input to an output based on example input-output pairs. It infers a function from labeled training data consisting of a set of training examples."
categories: [Machine Learning]
use_math: true
comments: true
tag: [AI, 파이썬, Python, Machine Learning]
---

## 머신러닝

* 머신러닝에서는 크게 지도학습(Supervised learning), 비지도 학습(Unsupervised Learning), 강화 학습(Reinforcement Learning)으로 나눌 수 있다.

![](https://blog.kakaocdn.net/dn/bUwxcG/btqAyyPLLl6/VAmK70YGuVVZ2pN05zf95k/img.png){: width="50%" height="50%"}

위 그림과 같이 지도 학습에는 Classification(분류)와 Regression(회귀)로 나눌수 있다.
* 분류
  1. 미리 정의된 여러 클래스 레이블 중 하나를 예측하는 것.
  2. 속성 값을 입력, 클래스 값을 출력으로 하는 모델
  3. 붓꽃(iris)의 세 품종 중 하나로 분류, 암 분류 등.
  4. 이진분류, 다중 분류 등이 있다.
* 회귀
  1. 연속적인 숫자를 예측하는 것.
  2. 속성 값을 입력, 연속적인 실수 값을 출력으로 하는 모델
  3. 어떤 사람의 교육수준, 나이, 주거지를 바탕으로 연간 소득 예측.
  4. 예측 값의 미묘한 차이가 크게 중요하지 않다.


Classification     |  Regression  
:------------:|:---------------------:
kNN           |  Linear Regression
Naive Bayes  |  Locally Weighted Linear
upport Vecto | Locally Weighted Linear
Support Vector| Ridge
Machine Decision | Lasso

지도학습 중에서 K-Nearest Neighbors (KNN, K최근접 알고리즘) 대해서 확인해 보겠습니다.

## K-Nearest Neighbors (KNN, K최근접 알고리즘)

 * 새로운 데이터 포인트와 가장 가까운 훈련 데이터세트의 데이터 포인트를 찾아 예측
 * k 값에 따라 가까운 이웃의 수가 결정
 * 분류와 회귀에 모두 사용 가능

 * 입력 값과 k개의 가까운 점이 있다고 가정할 때 그 점들이 어떤 라벨과 가장 비슷한지 (최근접 이웃) 판단하는 알고리즘

 * 매개 변수 : 데이터 포인트 사이의 거리를 재는 방법 (일반적으로 유클리디안 거리를 이용), 이웃의 수

 * 장점 : 이해하기 쉬운 모델, 약간의 조정으로 좋은 성능
 * 단점 : 훈련 세트가 크면 속도가 느림, 많은 특성을 처리하기 힘듬.

{% highlight python %}
import mglearn
mglearn.plots.plot_knn_classification(n_neighbors=1)
{% endhighlight %}
![](https://github.com/kimjinhyuk/kimjinhyuk.github.io/blob/master/knn01.PNG?raw=true)

{% highlight python %}
mglearn.plots.plot_knn_classification(n_neighbors=3)
{% endhighlight %}
![](https://github.com/kimjinhyuk/kimjinhyuk.github.io/blob/master/knn02.PNG?raw=true)

* k 값이 작을 수록 모델의 복잡도가 상대적으로 증가. (이웃을 적게 사용하면)
* 반대로 k 값이 커질수록 모델의 복잡도가 낮아진다. (이웃을 많이 사용하면)
* 100개의 데이터를 학습하고, k를 100개로 설정하여 예측하면 빈도가 가장 많은 클래스 레이블로 분류 (훈련 데이터 전체 개수를 이웃의 수로 지정하는 극단적인 경우)

* seaborn, sklearn 패키지를 이용한 KNN 분류 실습
{% highlight python %}
import seaborn as sns
from sklearn.model_selection import train_test_split
from sklearn.neighbors import KNeighborsClassifier
from sklearn import metrics

# 데이터 가져오기
# iris data loading
iris = sns.load_dataset('iris')
iris.shape

# 훈련 데이터와 테스트 데이터 준비
# 75% : 25%
X_train, X_test, y_train, y_test = train_test_split(
                                      iris.iloc[:,:4],
                                      iris.iloc[:,-1],
                                      random_state=0 )  # 데이터 섞음

# 모델 선택과 학습
# k-최근접 이웃 알고리즘
model = KNeighborsClassifier(n_neighbors=1)
model.fit(X_train, y_train)
pred = model.predict(X_test)
ac_score = metrics.accuracy_score(y_test, pred)
print("정답률 =", ac_score)

# 예측활용
X_new = [[5, 2.9, 1, 0.2]]
pre = model.predict(X_new)
print("예측:", pre)
{% endhighlight %}

{% highlight python %}
정답률 = 0.9736842105263158
예측: ['setosa']
{% endhighlight %}
